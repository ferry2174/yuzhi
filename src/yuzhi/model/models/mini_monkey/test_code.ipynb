{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InternLM2Tokenizer(name_or_path='/home/zhangjingbo/Workspace/Mini-Monkey', vocab_size=92544, model_max_length=8192, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|action_start|>', '<|action_end|>', '<|interpreter|>', '<|plugin|>', '<img>', '</img>', '<IMG_CONTEXT>', '<quad>', '</quad>', '<ref>', '</ref>', '<box>', '</box>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92538: AddedToken(\"<|plugin|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92539: AddedToken(\"<|interpreter|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92540: AddedToken(\"<|action_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92541: AddedToken(\"<|action_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92542: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92543: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92544: AddedToken(\"<img>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92545: AddedToken(\"</img>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92546: AddedToken(\"<IMG_CONTEXT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92547: AddedToken(\"<quad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92548: AddedToken(\"</quad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92549: AddedToken(\"<ref>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92550: AddedToken(\"</ref>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92551: AddedToken(\"<box>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92552: AddedToken(\"</box>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from tokenization_internlm2 import InternLM2Tokenizer\n",
    "\n",
    "checkpoint_path = \"/home/zhangjingbo/Workspace/Mini-Monkey\"\n",
    "\n",
    "tokenizer = InternLM2Tokenizer.from_pretrained(checkpoint_path)\n",
    "print(tokenizer)\n",
    "#print(tokenizer.tokenize(\"Hello, my name is John.\"))\n",
    "#print(tokenizer.encode(\"Hello, my name is John.\"))\n",
    "#print(tokenizer.decode(tokenizer.encode(\"Hello, my name is John.\")))\n",
    "#\n",
    "#save_tokenizer_path = \"/home/zhangjingbo/Workspace/Mini-Monkey/tokenizer\"\n",
    "#tokenizer.save_pretrained(save_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architectures : ['InternVisionModel']\n",
      "architecture_type : None\n",
      "model_type : intern_vit_6b\n",
      "yuzhi_version : None\n",
      "_name_or_path : \n",
      "return_dict : True\n",
      "add_cross_attention : False\n",
      "bad_words_ids : None\n",
      "begin_suppress_tokens : None\n",
      "bos_token_id : None\n",
      "chunk_size_feed_forward : 0\n",
      "cross_attention_hidden_size : None\n",
      "decoder_start_token_id : None\n",
      "diversity_penalty : 0.0\n",
      "do_sample : False\n",
      "early_stopping : False\n",
      "encoder_no_repeat_ngram_size : 0\n",
      "eos_token_id : None\n",
      "exponential_decay_length_penalty : None\n",
      "finetuning_task : None\n",
      "forced_bos_token_id : None\n",
      "forced_eos_token_id : None\n",
      "id2label : {'0': 'LABEL_0', '1': 'LABEL_1'}\n",
      "is_decoder : False\n",
      "is_encoder_decoder : False\n",
      "label2id : {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "length_penalty : 1.0\n",
      "max_length : 20\n",
      "min_length : 0\n",
      "no_repeat_ngram_size : 0\n",
      "num_beam_groups : 1\n",
      "num_beams : 1\n",
      "num_return_sequences : 1\n",
      "output_attentions : False\n",
      "output_hidden_states : False\n",
      "output_scores : False\n",
      "pad_token_id : None\n",
      "prefix : None\n",
      "problem_type : None\n",
      "pruned_heads : {}\n",
      "remove_invalid_values : False\n",
      "repetition_penalty : 1.0\n",
      "return_dict_in_generate : False\n",
      "sep_token_id : None\n",
      "suppress_tokens : None\n",
      "task_specific_params : None\n",
      "temperature : 1.0\n",
      "tf_legacy_loss : False\n",
      "tie_encoder_decoder : False\n",
      "tie_word_embeddings : True\n",
      "tokenizer_class : None\n",
      "top_k : 50\n",
      "top_p : None\n",
      "torch_dtype : bfloat16\n",
      "torchscript : False\n",
      "transformers_version : 4.37.2\n",
      "typical_p : 1.0\n",
      "use_bfloat16 : True\n",
      "hidden_size : 1024\n",
      "intermediate_size : 4096\n",
      "dropout : 0.0\n",
      "drop_path_rate : 0.0\n",
      "num_hidden_layers : 24\n",
      "num_attention_heads : 16\n",
      "num_channels : 3\n",
      "patch_size : 14\n",
      "image_size : 448\n",
      "initializer_range : 0.02\n",
      "initializer_factor : 1.0\n",
      "attention_dropout : 0.0\n",
      "layer_norm_eps : 1e-06\n",
      "hidden_act : gelu\n",
      "norm_type : layer_norm\n",
      "qkv_bias : True\n",
      "qk_normalization : False\n",
      "use_flash_attn : True\n"
     ]
    }
   ],
   "source": [
    "# from configuration_internvl_chat import InternVLChatConfig\n",
    "from configuration_intern_vit import InternVisionConfig\n",
    "# from configuration_internlm2 import InternLM2Config\n",
    "\n",
    "checkpoint_path = \"/home/zhangjingbo/Workspace/Mini-Monkey\"\n",
    "\n",
    "# config_vision = InternVisionConfig.from_pretrained(checkpoint_path)\n",
    "# config_lm2 = InternLM2Config.from_pretrained(checkpoint_path)\n",
    "# config_chat = InternVLChatConfig.from_pretrained(checkpoint_path)\n",
    "\n",
    "kwargs = {}\n",
    "config_vision, unused_kwargs = InternVisionConfig.from_pretrained(\n",
    "                checkpoint_path,\n",
    "                return_unused_kwargs=True,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "for key, value in config_vision.__dict__.items():\n",
    "    print(key, \":\", value)\n",
    "\n",
    "# for key, value in config_lm2.__dict__.items():\n",
    "#     print(key, \":\", value)\n",
    "\n",
    "# for key, value in config_chat.__dict__.items():\n",
    "#     print(key, \":\", value)\n",
    "#     if key == \"vision_config\":\n",
    "#         for k, v in value.__dict__.items():\n",
    "#             print(\"vision_config.\", k, \":\", v)\n",
    "#     elif key == \"lm2_config\":\n",
    "#         for k, v in value.__dict__.items():\n",
    "#             print(\"lm2_config.\", k, \":\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/zhangjingbo/Workspace/Mini-Monkey were not used when initializing InternVisionModel: ['language_model.model.layers.0.attention.wo.weight', 'language_model.model.layers.0.attention.wqkv.weight', 'language_model.model.layers.0.attention_norm.weight', 'language_model.model.layers.0.feed_forward.w1.weight', 'language_model.model.layers.0.feed_forward.w2.weight', 'language_model.model.layers.0.feed_forward.w3.weight', 'language_model.model.layers.0.ffn_norm.weight', 'language_model.model.layers.1.attention.wo.weight', 'language_model.model.layers.1.attention.wqkv.weight', 'language_model.model.layers.1.attention_norm.weight', 'language_model.model.layers.1.feed_forward.w1.weight', 'language_model.model.layers.1.feed_forward.w2.weight', 'language_model.model.layers.1.feed_forward.w3.weight', 'language_model.model.layers.1.ffn_norm.weight', 'language_model.model.layers.10.attention.wo.weight', 'language_model.model.layers.10.attention.wqkv.weight', 'language_model.model.layers.10.attention_norm.weight', 'language_model.model.layers.10.feed_forward.w1.weight', 'language_model.model.layers.10.feed_forward.w2.weight', 'language_model.model.layers.10.feed_forward.w3.weight', 'language_model.model.layers.10.ffn_norm.weight', 'language_model.model.layers.11.attention.wo.weight', 'language_model.model.layers.11.attention.wqkv.weight', 'language_model.model.layers.11.attention_norm.weight', 'language_model.model.layers.11.feed_forward.w1.weight', 'language_model.model.layers.11.feed_forward.w2.weight', 'language_model.model.layers.11.feed_forward.w3.weight', 'language_model.model.layers.11.ffn_norm.weight', 'language_model.model.layers.12.attention.wo.weight', 'language_model.model.layers.12.attention.wqkv.weight', 'language_model.model.layers.12.attention_norm.weight', 'language_model.model.layers.12.feed_forward.w1.weight', 'language_model.model.layers.12.feed_forward.w2.weight', 'language_model.model.layers.12.feed_forward.w3.weight', 'language_model.model.layers.12.ffn_norm.weight', 'language_model.model.layers.13.attention.wo.weight', 'language_model.model.layers.13.attention.wqkv.weight', 'language_model.model.layers.13.attention_norm.weight', 'language_model.model.layers.13.feed_forward.w1.weight', 'language_model.model.layers.13.feed_forward.w2.weight', 'language_model.model.layers.13.feed_forward.w3.weight', 'language_model.model.layers.13.ffn_norm.weight', 'language_model.model.layers.14.attention.wo.weight', 'language_model.model.layers.14.attention.wqkv.weight', 'language_model.model.layers.14.attention_norm.weight', 'language_model.model.layers.14.feed_forward.w1.weight', 'language_model.model.layers.14.feed_forward.w2.weight', 'language_model.model.layers.14.feed_forward.w3.weight', 'language_model.model.layers.14.ffn_norm.weight', 'language_model.model.layers.15.attention.wo.weight', 'language_model.model.layers.15.attention.wqkv.weight', 'language_model.model.layers.15.attention_norm.weight', 'language_model.model.layers.15.feed_forward.w1.weight', 'language_model.model.layers.15.feed_forward.w2.weight', 'language_model.model.layers.15.feed_forward.w3.weight', 'language_model.model.layers.15.ffn_norm.weight', 'language_model.model.layers.16.attention.wo.weight', 'language_model.model.layers.16.attention.wqkv.weight', 'language_model.model.layers.16.attention_norm.weight', 'language_model.model.layers.16.feed_forward.w1.weight', 'language_model.model.layers.16.feed_forward.w2.weight', 'language_model.model.layers.16.feed_forward.w3.weight', 'language_model.model.layers.16.ffn_norm.weight', 'language_model.model.layers.17.attention.wo.weight', 'language_model.model.layers.17.attention.wqkv.weight', 'language_model.model.layers.17.attention_norm.weight', 'language_model.model.layers.17.feed_forward.w1.weight', 'language_model.model.layers.17.feed_forward.w2.weight', 'language_model.model.layers.17.feed_forward.w3.weight', 'language_model.model.layers.17.ffn_norm.weight', 'language_model.model.layers.18.attention.wo.weight', 'language_model.model.layers.18.attention.wqkv.weight', 'language_model.model.layers.18.attention_norm.weight', 'language_model.model.layers.18.feed_forward.w1.weight', 'language_model.model.layers.18.feed_forward.w2.weight', 'language_model.model.layers.18.feed_forward.w3.weight', 'language_model.model.layers.18.ffn_norm.weight', 'language_model.model.layers.19.attention.wo.weight', 'language_model.model.layers.19.attention.wqkv.weight', 'language_model.model.layers.19.attention_norm.weight', 'language_model.model.layers.19.feed_forward.w1.weight', 'language_model.model.layers.19.feed_forward.w2.weight', 'language_model.model.layers.19.feed_forward.w3.weight', 'language_model.model.layers.19.ffn_norm.weight', 'language_model.model.layers.2.attention.wo.weight', 'language_model.model.layers.2.attention.wqkv.weight', 'language_model.model.layers.2.attention_norm.weight', 'language_model.model.layers.2.feed_forward.w1.weight', 'language_model.model.layers.2.feed_forward.w2.weight', 'language_model.model.layers.2.feed_forward.w3.weight', 'language_model.model.layers.2.ffn_norm.weight', 'language_model.model.layers.20.attention.wo.weight', 'language_model.model.layers.20.attention.wqkv.weight', 'language_model.model.layers.20.attention_norm.weight', 'language_model.model.layers.20.feed_forward.w1.weight', 'language_model.model.layers.20.feed_forward.w2.weight', 'language_model.model.layers.20.feed_forward.w3.weight', 'language_model.model.layers.20.ffn_norm.weight', 'language_model.model.layers.21.attention.wo.weight', 'language_model.model.layers.21.attention.wqkv.weight', 'language_model.model.layers.21.attention_norm.weight', 'language_model.model.layers.21.feed_forward.w1.weight', 'language_model.model.layers.21.feed_forward.w2.weight', 'language_model.model.layers.21.feed_forward.w3.weight', 'language_model.model.layers.21.ffn_norm.weight', 'language_model.model.layers.22.attention.wo.weight', 'language_model.model.layers.22.attention.wqkv.weight', 'language_model.model.layers.22.attention_norm.weight', 'language_model.model.layers.22.feed_forward.w1.weight', 'language_model.model.layers.22.feed_forward.w2.weight', 'language_model.model.layers.22.feed_forward.w3.weight', 'language_model.model.layers.22.ffn_norm.weight', 'language_model.model.layers.23.attention.wo.weight', 'language_model.model.layers.23.attention.wqkv.weight', 'language_model.model.layers.23.attention_norm.weight', 'language_model.model.layers.23.feed_forward.w1.weight', 'language_model.model.layers.23.feed_forward.w2.weight', 'language_model.model.layers.23.feed_forward.w3.weight', 'language_model.model.layers.23.ffn_norm.weight', 'language_model.model.layers.3.attention.wo.weight', 'language_model.model.layers.3.attention.wqkv.weight', 'language_model.model.layers.3.attention_norm.weight', 'language_model.model.layers.3.feed_forward.w1.weight', 'language_model.model.layers.3.feed_forward.w2.weight', 'language_model.model.layers.3.feed_forward.w3.weight', 'language_model.model.layers.3.ffn_norm.weight', 'language_model.model.layers.4.attention.wo.weight', 'language_model.model.layers.4.attention.wqkv.weight', 'language_model.model.layers.4.attention_norm.weight', 'language_model.model.layers.4.feed_forward.w1.weight', 'language_model.model.layers.4.feed_forward.w2.weight', 'language_model.model.layers.4.feed_forward.w3.weight', 'language_model.model.layers.4.ffn_norm.weight', 'language_model.model.layers.5.attention.wo.weight', 'language_model.model.layers.5.attention.wqkv.weight', 'language_model.model.layers.5.attention_norm.weight', 'language_model.model.layers.5.feed_forward.w1.weight', 'language_model.model.layers.5.feed_forward.w2.weight', 'language_model.model.layers.5.feed_forward.w3.weight', 'language_model.model.layers.5.ffn_norm.weight', 'language_model.model.layers.6.attention.wo.weight', 'language_model.model.layers.6.attention.wqkv.weight', 'language_model.model.layers.6.attention_norm.weight', 'language_model.model.layers.6.feed_forward.w1.weight', 'language_model.model.layers.6.feed_forward.w2.weight', 'language_model.model.layers.6.feed_forward.w3.weight', 'language_model.model.layers.6.ffn_norm.weight', 'language_model.model.layers.7.attention.wo.weight', 'language_model.model.layers.7.attention.wqkv.weight', 'language_model.model.layers.7.attention_norm.weight', 'language_model.model.layers.7.feed_forward.w1.weight', 'language_model.model.layers.7.feed_forward.w2.weight', 'language_model.model.layers.7.feed_forward.w3.weight', 'language_model.model.layers.7.ffn_norm.weight', 'language_model.model.layers.8.attention.wo.weight', 'language_model.model.layers.8.attention.wqkv.weight', 'language_model.model.layers.8.attention_norm.weight', 'language_model.model.layers.8.feed_forward.w1.weight', 'language_model.model.layers.8.feed_forward.w2.weight', 'language_model.model.layers.8.feed_forward.w3.weight', 'language_model.model.layers.8.ffn_norm.weight', 'language_model.model.layers.9.attention.wo.weight', 'language_model.model.layers.9.attention.wqkv.weight', 'language_model.model.layers.9.attention_norm.weight', 'language_model.model.layers.9.feed_forward.w1.weight', 'language_model.model.layers.9.feed_forward.w2.weight', 'language_model.model.layers.9.feed_forward.w3.weight', 'language_model.model.layers.9.ffn_norm.weight', 'language_model.model.norm.weight', 'language_model.model.tok_embeddings.weight', 'language_model.output.weight', 'mlp1.0.bias', 'mlp1.0.weight', 'mlp1.1.bias', 'mlp1.1.weight', 'mlp1.3.bias', 'mlp1.3.weight', 'vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.attn.proj.bias', 'vision_model.encoder.layers.0.attn.proj.weight', 'vision_model.encoder.layers.0.attn.qkv.bias', 'vision_model.encoder.layers.0.attn.qkv.weight', 'vision_model.encoder.layers.0.ls1', 'vision_model.encoder.layers.0.ls2', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.norm1.bias', 'vision_model.encoder.layers.0.norm1.weight', 'vision_model.encoder.layers.0.norm2.bias', 'vision_model.encoder.layers.0.norm2.weight', 'vision_model.encoder.layers.1.attn.proj.bias', 'vision_model.encoder.layers.1.attn.proj.weight', 'vision_model.encoder.layers.1.attn.qkv.bias', 'vision_model.encoder.layers.1.attn.qkv.weight', 'vision_model.encoder.layers.1.ls1', 'vision_model.encoder.layers.1.ls2', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.norm1.bias', 'vision_model.encoder.layers.1.norm1.weight', 'vision_model.encoder.layers.1.norm2.bias', 'vision_model.encoder.layers.1.norm2.weight', 'vision_model.encoder.layers.10.attn.proj.bias', 'vision_model.encoder.layers.10.attn.proj.weight', 'vision_model.encoder.layers.10.attn.qkv.bias', 'vision_model.encoder.layers.10.attn.qkv.weight', 'vision_model.encoder.layers.10.ls1', 'vision_model.encoder.layers.10.ls2', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.norm1.bias', 'vision_model.encoder.layers.10.norm1.weight', 'vision_model.encoder.layers.10.norm2.bias', 'vision_model.encoder.layers.10.norm2.weight', 'vision_model.encoder.layers.11.attn.proj.bias', 'vision_model.encoder.layers.11.attn.proj.weight', 'vision_model.encoder.layers.11.attn.qkv.bias', 'vision_model.encoder.layers.11.attn.qkv.weight', 'vision_model.encoder.layers.11.ls1', 'vision_model.encoder.layers.11.ls2', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.norm1.bias', 'vision_model.encoder.layers.11.norm1.weight', 'vision_model.encoder.layers.11.norm2.bias', 'vision_model.encoder.layers.11.norm2.weight', 'vision_model.encoder.layers.12.attn.proj.bias', 'vision_model.encoder.layers.12.attn.proj.weight', 'vision_model.encoder.layers.12.attn.qkv.bias', 'vision_model.encoder.layers.12.attn.qkv.weight', 'vision_model.encoder.layers.12.ls1', 'vision_model.encoder.layers.12.ls2', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.norm1.bias', 'vision_model.encoder.layers.12.norm1.weight', 'vision_model.encoder.layers.12.norm2.bias', 'vision_model.encoder.layers.12.norm2.weight', 'vision_model.encoder.layers.13.attn.proj.bias', 'vision_model.encoder.layers.13.attn.proj.weight', 'vision_model.encoder.layers.13.attn.qkv.bias', 'vision_model.encoder.layers.13.attn.qkv.weight', 'vision_model.encoder.layers.13.ls1', 'vision_model.encoder.layers.13.ls2', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.norm1.bias', 'vision_model.encoder.layers.13.norm1.weight', 'vision_model.encoder.layers.13.norm2.bias', 'vision_model.encoder.layers.13.norm2.weight', 'vision_model.encoder.layers.14.attn.proj.bias', 'vision_model.encoder.layers.14.attn.proj.weight', 'vision_model.encoder.layers.14.attn.qkv.bias', 'vision_model.encoder.layers.14.attn.qkv.weight', 'vision_model.encoder.layers.14.ls1', 'vision_model.encoder.layers.14.ls2', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.norm1.bias', 'vision_model.encoder.layers.14.norm1.weight', 'vision_model.encoder.layers.14.norm2.bias', 'vision_model.encoder.layers.14.norm2.weight', 'vision_model.encoder.layers.15.attn.proj.bias', 'vision_model.encoder.layers.15.attn.proj.weight', 'vision_model.encoder.layers.15.attn.qkv.bias', 'vision_model.encoder.layers.15.attn.qkv.weight', 'vision_model.encoder.layers.15.ls1', 'vision_model.encoder.layers.15.ls2', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.norm1.bias', 'vision_model.encoder.layers.15.norm1.weight', 'vision_model.encoder.layers.15.norm2.bias', 'vision_model.encoder.layers.15.norm2.weight', 'vision_model.encoder.layers.16.attn.proj.bias', 'vision_model.encoder.layers.16.attn.proj.weight', 'vision_model.encoder.layers.16.attn.qkv.bias', 'vision_model.encoder.layers.16.attn.qkv.weight', 'vision_model.encoder.layers.16.ls1', 'vision_model.encoder.layers.16.ls2', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.norm1.bias', 'vision_model.encoder.layers.16.norm1.weight', 'vision_model.encoder.layers.16.norm2.bias', 'vision_model.encoder.layers.16.norm2.weight', 'vision_model.encoder.layers.17.attn.proj.bias', 'vision_model.encoder.layers.17.attn.proj.weight', 'vision_model.encoder.layers.17.attn.qkv.bias', 'vision_model.encoder.layers.17.attn.qkv.weight', 'vision_model.encoder.layers.17.ls1', 'vision_model.encoder.layers.17.ls2', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.norm1.bias', 'vision_model.encoder.layers.17.norm1.weight', 'vision_model.encoder.layers.17.norm2.bias', 'vision_model.encoder.layers.17.norm2.weight', 'vision_model.encoder.layers.18.attn.proj.bias', 'vision_model.encoder.layers.18.attn.proj.weight', 'vision_model.encoder.layers.18.attn.qkv.bias', 'vision_model.encoder.layers.18.attn.qkv.weight', 'vision_model.encoder.layers.18.ls1', 'vision_model.encoder.layers.18.ls2', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.norm1.bias', 'vision_model.encoder.layers.18.norm1.weight', 'vision_model.encoder.layers.18.norm2.bias', 'vision_model.encoder.layers.18.norm2.weight', 'vision_model.encoder.layers.19.attn.proj.bias', 'vision_model.encoder.layers.19.attn.proj.weight', 'vision_model.encoder.layers.19.attn.qkv.bias', 'vision_model.encoder.layers.19.attn.qkv.weight', 'vision_model.encoder.layers.19.ls1', 'vision_model.encoder.layers.19.ls2', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.norm1.bias', 'vision_model.encoder.layers.19.norm1.weight', 'vision_model.encoder.layers.19.norm2.bias', 'vision_model.encoder.layers.19.norm2.weight', 'vision_model.encoder.layers.2.attn.proj.bias', 'vision_model.encoder.layers.2.attn.proj.weight', 'vision_model.encoder.layers.2.attn.qkv.bias', 'vision_model.encoder.layers.2.attn.qkv.weight', 'vision_model.encoder.layers.2.ls1', 'vision_model.encoder.layers.2.ls2', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.norm1.bias', 'vision_model.encoder.layers.2.norm1.weight', 'vision_model.encoder.layers.2.norm2.bias', 'vision_model.encoder.layers.2.norm2.weight', 'vision_model.encoder.layers.20.attn.proj.bias', 'vision_model.encoder.layers.20.attn.proj.weight', 'vision_model.encoder.layers.20.attn.qkv.bias', 'vision_model.encoder.layers.20.attn.qkv.weight', 'vision_model.encoder.layers.20.ls1', 'vision_model.encoder.layers.20.ls2', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.norm1.bias', 'vision_model.encoder.layers.20.norm1.weight', 'vision_model.encoder.layers.20.norm2.bias', 'vision_model.encoder.layers.20.norm2.weight', 'vision_model.encoder.layers.21.attn.proj.bias', 'vision_model.encoder.layers.21.attn.proj.weight', 'vision_model.encoder.layers.21.attn.qkv.bias', 'vision_model.encoder.layers.21.attn.qkv.weight', 'vision_model.encoder.layers.21.ls1', 'vision_model.encoder.layers.21.ls2', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.norm1.bias', 'vision_model.encoder.layers.21.norm1.weight', 'vision_model.encoder.layers.21.norm2.bias', 'vision_model.encoder.layers.21.norm2.weight', 'vision_model.encoder.layers.22.attn.proj.bias', 'vision_model.encoder.layers.22.attn.proj.weight', 'vision_model.encoder.layers.22.attn.qkv.bias', 'vision_model.encoder.layers.22.attn.qkv.weight', 'vision_model.encoder.layers.22.ls1', 'vision_model.encoder.layers.22.ls2', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.norm1.bias', 'vision_model.encoder.layers.22.norm1.weight', 'vision_model.encoder.layers.22.norm2.bias', 'vision_model.encoder.layers.22.norm2.weight', 'vision_model.encoder.layers.23.attn.proj.bias', 'vision_model.encoder.layers.23.attn.proj.weight', 'vision_model.encoder.layers.23.attn.qkv.bias', 'vision_model.encoder.layers.23.attn.qkv.weight', 'vision_model.encoder.layers.23.ls1', 'vision_model.encoder.layers.23.ls2', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.norm1.bias', 'vision_model.encoder.layers.23.norm1.weight', 'vision_model.encoder.layers.23.norm2.bias', 'vision_model.encoder.layers.23.norm2.weight', 'vision_model.encoder.layers.3.attn.proj.bias', 'vision_model.encoder.layers.3.attn.proj.weight', 'vision_model.encoder.layers.3.attn.qkv.bias', 'vision_model.encoder.layers.3.attn.qkv.weight', 'vision_model.encoder.layers.3.ls1', 'vision_model.encoder.layers.3.ls2', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.norm1.bias', 'vision_model.encoder.layers.3.norm1.weight', 'vision_model.encoder.layers.3.norm2.bias', 'vision_model.encoder.layers.3.norm2.weight', 'vision_model.encoder.layers.4.attn.proj.bias', 'vision_model.encoder.layers.4.attn.proj.weight', 'vision_model.encoder.layers.4.attn.qkv.bias', 'vision_model.encoder.layers.4.attn.qkv.weight', 'vision_model.encoder.layers.4.ls1', 'vision_model.encoder.layers.4.ls2', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.norm1.bias', 'vision_model.encoder.layers.4.norm1.weight', 'vision_model.encoder.layers.4.norm2.bias', 'vision_model.encoder.layers.4.norm2.weight', 'vision_model.encoder.layers.5.attn.proj.bias', 'vision_model.encoder.layers.5.attn.proj.weight', 'vision_model.encoder.layers.5.attn.qkv.bias', 'vision_model.encoder.layers.5.attn.qkv.weight', 'vision_model.encoder.layers.5.ls1', 'vision_model.encoder.layers.5.ls2', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.norm1.bias', 'vision_model.encoder.layers.5.norm1.weight', 'vision_model.encoder.layers.5.norm2.bias', 'vision_model.encoder.layers.5.norm2.weight', 'vision_model.encoder.layers.6.attn.proj.bias', 'vision_model.encoder.layers.6.attn.proj.weight', 'vision_model.encoder.layers.6.attn.qkv.bias', 'vision_model.encoder.layers.6.attn.qkv.weight', 'vision_model.encoder.layers.6.ls1', 'vision_model.encoder.layers.6.ls2', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.norm1.bias', 'vision_model.encoder.layers.6.norm1.weight', 'vision_model.encoder.layers.6.norm2.bias', 'vision_model.encoder.layers.6.norm2.weight', 'vision_model.encoder.layers.7.attn.proj.bias', 'vision_model.encoder.layers.7.attn.proj.weight', 'vision_model.encoder.layers.7.attn.qkv.bias', 'vision_model.encoder.layers.7.attn.qkv.weight', 'vision_model.encoder.layers.7.ls1', 'vision_model.encoder.layers.7.ls2', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.norm1.bias', 'vision_model.encoder.layers.7.norm1.weight', 'vision_model.encoder.layers.7.norm2.bias', 'vision_model.encoder.layers.7.norm2.weight', 'vision_model.encoder.layers.8.attn.proj.bias', 'vision_model.encoder.layers.8.attn.proj.weight', 'vision_model.encoder.layers.8.attn.qkv.bias', 'vision_model.encoder.layers.8.attn.qkv.weight', 'vision_model.encoder.layers.8.ls1', 'vision_model.encoder.layers.8.ls2', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.norm1.bias', 'vision_model.encoder.layers.8.norm1.weight', 'vision_model.encoder.layers.8.norm2.bias', 'vision_model.encoder.layers.8.norm2.weight', 'vision_model.encoder.layers.9.attn.proj.bias', 'vision_model.encoder.layers.9.attn.proj.weight', 'vision_model.encoder.layers.9.attn.qkv.bias', 'vision_model.encoder.layers.9.attn.qkv.weight', 'vision_model.encoder.layers.9.ls1', 'vision_model.encoder.layers.9.ls2', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.norm1.bias', 'vision_model.encoder.layers.9.norm1.weight', 'vision_model.encoder.layers.9.norm2.bias', 'vision_model.encoder.layers.9.norm2.weight']\n",
      "- This IS expected if you are initializing InternVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing InternVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of InternVisionModel were not initialized from the model checkpoint at /home/zhangjingbo/Workspace/Mini-Monkey and are newly initialized: ['embeddings.class_embedding', 'embeddings.patch_embedding.bias', 'embeddings.patch_embedding.weight', 'embeddings.position_embedding', 'encoder.layers.0.attn.proj.bias', 'encoder.layers.0.attn.proj.weight', 'encoder.layers.0.attn.qkv.bias', 'encoder.layers.0.attn.qkv.weight', 'encoder.layers.0.ls1', 'encoder.layers.0.ls2', 'encoder.layers.0.mlp.fc1.bias', 'encoder.layers.0.mlp.fc1.weight', 'encoder.layers.0.mlp.fc2.bias', 'encoder.layers.0.mlp.fc2.weight', 'encoder.layers.0.norm1.bias', 'encoder.layers.0.norm1.weight', 'encoder.layers.0.norm2.bias', 'encoder.layers.0.norm2.weight', 'encoder.layers.1.attn.proj.bias', 'encoder.layers.1.attn.proj.weight', 'encoder.layers.1.attn.qkv.bias', 'encoder.layers.1.attn.qkv.weight', 'encoder.layers.1.ls1', 'encoder.layers.1.ls2', 'encoder.layers.1.mlp.fc1.bias', 'encoder.layers.1.mlp.fc1.weight', 'encoder.layers.1.mlp.fc2.bias', 'encoder.layers.1.mlp.fc2.weight', 'encoder.layers.1.norm1.bias', 'encoder.layers.1.norm1.weight', 'encoder.layers.1.norm2.bias', 'encoder.layers.1.norm2.weight', 'encoder.layers.10.attn.proj.bias', 'encoder.layers.10.attn.proj.weight', 'encoder.layers.10.attn.qkv.bias', 'encoder.layers.10.attn.qkv.weight', 'encoder.layers.10.ls1', 'encoder.layers.10.ls2', 'encoder.layers.10.mlp.fc1.bias', 'encoder.layers.10.mlp.fc1.weight', 'encoder.layers.10.mlp.fc2.bias', 'encoder.layers.10.mlp.fc2.weight', 'encoder.layers.10.norm1.bias', 'encoder.layers.10.norm1.weight', 'encoder.layers.10.norm2.bias', 'encoder.layers.10.norm2.weight', 'encoder.layers.11.attn.proj.bias', 'encoder.layers.11.attn.proj.weight', 'encoder.layers.11.attn.qkv.bias', 'encoder.layers.11.attn.qkv.weight', 'encoder.layers.11.ls1', 'encoder.layers.11.ls2', 'encoder.layers.11.mlp.fc1.bias', 'encoder.layers.11.mlp.fc1.weight', 'encoder.layers.11.mlp.fc2.bias', 'encoder.layers.11.mlp.fc2.weight', 'encoder.layers.11.norm1.bias', 'encoder.layers.11.norm1.weight', 'encoder.layers.11.norm2.bias', 'encoder.layers.11.norm2.weight', 'encoder.layers.12.attn.proj.bias', 'encoder.layers.12.attn.proj.weight', 'encoder.layers.12.attn.qkv.bias', 'encoder.layers.12.attn.qkv.weight', 'encoder.layers.12.ls1', 'encoder.layers.12.ls2', 'encoder.layers.12.mlp.fc1.bias', 'encoder.layers.12.mlp.fc1.weight', 'encoder.layers.12.mlp.fc2.bias', 'encoder.layers.12.mlp.fc2.weight', 'encoder.layers.12.norm1.bias', 'encoder.layers.12.norm1.weight', 'encoder.layers.12.norm2.bias', 'encoder.layers.12.norm2.weight', 'encoder.layers.13.attn.proj.bias', 'encoder.layers.13.attn.proj.weight', 'encoder.layers.13.attn.qkv.bias', 'encoder.layers.13.attn.qkv.weight', 'encoder.layers.13.ls1', 'encoder.layers.13.ls2', 'encoder.layers.13.mlp.fc1.bias', 'encoder.layers.13.mlp.fc1.weight', 'encoder.layers.13.mlp.fc2.bias', 'encoder.layers.13.mlp.fc2.weight', 'encoder.layers.13.norm1.bias', 'encoder.layers.13.norm1.weight', 'encoder.layers.13.norm2.bias', 'encoder.layers.13.norm2.weight', 'encoder.layers.14.attn.proj.bias', 'encoder.layers.14.attn.proj.weight', 'encoder.layers.14.attn.qkv.bias', 'encoder.layers.14.attn.qkv.weight', 'encoder.layers.14.ls1', 'encoder.layers.14.ls2', 'encoder.layers.14.mlp.fc1.bias', 'encoder.layers.14.mlp.fc1.weight', 'encoder.layers.14.mlp.fc2.bias', 'encoder.layers.14.mlp.fc2.weight', 'encoder.layers.14.norm1.bias', 'encoder.layers.14.norm1.weight', 'encoder.layers.14.norm2.bias', 'encoder.layers.14.norm2.weight', 'encoder.layers.15.attn.proj.bias', 'encoder.layers.15.attn.proj.weight', 'encoder.layers.15.attn.qkv.bias', 'encoder.layers.15.attn.qkv.weight', 'encoder.layers.15.ls1', 'encoder.layers.15.ls2', 'encoder.layers.15.mlp.fc1.bias', 'encoder.layers.15.mlp.fc1.weight', 'encoder.layers.15.mlp.fc2.bias', 'encoder.layers.15.mlp.fc2.weight', 'encoder.layers.15.norm1.bias', 'encoder.layers.15.norm1.weight', 'encoder.layers.15.norm2.bias', 'encoder.layers.15.norm2.weight', 'encoder.layers.16.attn.proj.bias', 'encoder.layers.16.attn.proj.weight', 'encoder.layers.16.attn.qkv.bias', 'encoder.layers.16.attn.qkv.weight', 'encoder.layers.16.ls1', 'encoder.layers.16.ls2', 'encoder.layers.16.mlp.fc1.bias', 'encoder.layers.16.mlp.fc1.weight', 'encoder.layers.16.mlp.fc2.bias', 'encoder.layers.16.mlp.fc2.weight', 'encoder.layers.16.norm1.bias', 'encoder.layers.16.norm1.weight', 'encoder.layers.16.norm2.bias', 'encoder.layers.16.norm2.weight', 'encoder.layers.17.attn.proj.bias', 'encoder.layers.17.attn.proj.weight', 'encoder.layers.17.attn.qkv.bias', 'encoder.layers.17.attn.qkv.weight', 'encoder.layers.17.ls1', 'encoder.layers.17.ls2', 'encoder.layers.17.mlp.fc1.bias', 'encoder.layers.17.mlp.fc1.weight', 'encoder.layers.17.mlp.fc2.bias', 'encoder.layers.17.mlp.fc2.weight', 'encoder.layers.17.norm1.bias', 'encoder.layers.17.norm1.weight', 'encoder.layers.17.norm2.bias', 'encoder.layers.17.norm2.weight', 'encoder.layers.18.attn.proj.bias', 'encoder.layers.18.attn.proj.weight', 'encoder.layers.18.attn.qkv.bias', 'encoder.layers.18.attn.qkv.weight', 'encoder.layers.18.ls1', 'encoder.layers.18.ls2', 'encoder.layers.18.mlp.fc1.bias', 'encoder.layers.18.mlp.fc1.weight', 'encoder.layers.18.mlp.fc2.bias', 'encoder.layers.18.mlp.fc2.weight', 'encoder.layers.18.norm1.bias', 'encoder.layers.18.norm1.weight', 'encoder.layers.18.norm2.bias', 'encoder.layers.18.norm2.weight', 'encoder.layers.19.attn.proj.bias', 'encoder.layers.19.attn.proj.weight', 'encoder.layers.19.attn.qkv.bias', 'encoder.layers.19.attn.qkv.weight', 'encoder.layers.19.ls1', 'encoder.layers.19.ls2', 'encoder.layers.19.mlp.fc1.bias', 'encoder.layers.19.mlp.fc1.weight', 'encoder.layers.19.mlp.fc2.bias', 'encoder.layers.19.mlp.fc2.weight', 'encoder.layers.19.norm1.bias', 'encoder.layers.19.norm1.weight', 'encoder.layers.19.norm2.bias', 'encoder.layers.19.norm2.weight', 'encoder.layers.2.attn.proj.bias', 'encoder.layers.2.attn.proj.weight', 'encoder.layers.2.attn.qkv.bias', 'encoder.layers.2.attn.qkv.weight', 'encoder.layers.2.ls1', 'encoder.layers.2.ls2', 'encoder.layers.2.mlp.fc1.bias', 'encoder.layers.2.mlp.fc1.weight', 'encoder.layers.2.mlp.fc2.bias', 'encoder.layers.2.mlp.fc2.weight', 'encoder.layers.2.norm1.bias', 'encoder.layers.2.norm1.weight', 'encoder.layers.2.norm2.bias', 'encoder.layers.2.norm2.weight', 'encoder.layers.20.attn.proj.bias', 'encoder.layers.20.attn.proj.weight', 'encoder.layers.20.attn.qkv.bias', 'encoder.layers.20.attn.qkv.weight', 'encoder.layers.20.ls1', 'encoder.layers.20.ls2', 'encoder.layers.20.mlp.fc1.bias', 'encoder.layers.20.mlp.fc1.weight', 'encoder.layers.20.mlp.fc2.bias', 'encoder.layers.20.mlp.fc2.weight', 'encoder.layers.20.norm1.bias', 'encoder.layers.20.norm1.weight', 'encoder.layers.20.norm2.bias', 'encoder.layers.20.norm2.weight', 'encoder.layers.21.attn.proj.bias', 'encoder.layers.21.attn.proj.weight', 'encoder.layers.21.attn.qkv.bias', 'encoder.layers.21.attn.qkv.weight', 'encoder.layers.21.ls1', 'encoder.layers.21.ls2', 'encoder.layers.21.mlp.fc1.bias', 'encoder.layers.21.mlp.fc1.weight', 'encoder.layers.21.mlp.fc2.bias', 'encoder.layers.21.mlp.fc2.weight', 'encoder.layers.21.norm1.bias', 'encoder.layers.21.norm1.weight', 'encoder.layers.21.norm2.bias', 'encoder.layers.21.norm2.weight', 'encoder.layers.22.attn.proj.bias', 'encoder.layers.22.attn.proj.weight', 'encoder.layers.22.attn.qkv.bias', 'encoder.layers.22.attn.qkv.weight', 'encoder.layers.22.ls1', 'encoder.layers.22.ls2', 'encoder.layers.22.mlp.fc1.bias', 'encoder.layers.22.mlp.fc1.weight', 'encoder.layers.22.mlp.fc2.bias', 'encoder.layers.22.mlp.fc2.weight', 'encoder.layers.22.norm1.bias', 'encoder.layers.22.norm1.weight', 'encoder.layers.22.norm2.bias', 'encoder.layers.22.norm2.weight', 'encoder.layers.23.attn.proj.bias', 'encoder.layers.23.attn.proj.weight', 'encoder.layers.23.attn.qkv.bias', 'encoder.layers.23.attn.qkv.weight', 'encoder.layers.23.ls1', 'encoder.layers.23.ls2', 'encoder.layers.23.mlp.fc1.bias', 'encoder.layers.23.mlp.fc1.weight', 'encoder.layers.23.mlp.fc2.bias', 'encoder.layers.23.mlp.fc2.weight', 'encoder.layers.23.norm1.bias', 'encoder.layers.23.norm1.weight', 'encoder.layers.23.norm2.bias', 'encoder.layers.23.norm2.weight', 'encoder.layers.3.attn.proj.bias', 'encoder.layers.3.attn.proj.weight', 'encoder.layers.3.attn.qkv.bias', 'encoder.layers.3.attn.qkv.weight', 'encoder.layers.3.ls1', 'encoder.layers.3.ls2', 'encoder.layers.3.mlp.fc1.bias', 'encoder.layers.3.mlp.fc1.weight', 'encoder.layers.3.mlp.fc2.bias', 'encoder.layers.3.mlp.fc2.weight', 'encoder.layers.3.norm1.bias', 'encoder.layers.3.norm1.weight', 'encoder.layers.3.norm2.bias', 'encoder.layers.3.norm2.weight', 'encoder.layers.4.attn.proj.bias', 'encoder.layers.4.attn.proj.weight', 'encoder.layers.4.attn.qkv.bias', 'encoder.layers.4.attn.qkv.weight', 'encoder.layers.4.ls1', 'encoder.layers.4.ls2', 'encoder.layers.4.mlp.fc1.bias', 'encoder.layers.4.mlp.fc1.weight', 'encoder.layers.4.mlp.fc2.bias', 'encoder.layers.4.mlp.fc2.weight', 'encoder.layers.4.norm1.bias', 'encoder.layers.4.norm1.weight', 'encoder.layers.4.norm2.bias', 'encoder.layers.4.norm2.weight', 'encoder.layers.5.attn.proj.bias', 'encoder.layers.5.attn.proj.weight', 'encoder.layers.5.attn.qkv.bias', 'encoder.layers.5.attn.qkv.weight', 'encoder.layers.5.ls1', 'encoder.layers.5.ls2', 'encoder.layers.5.mlp.fc1.bias', 'encoder.layers.5.mlp.fc1.weight', 'encoder.layers.5.mlp.fc2.bias', 'encoder.layers.5.mlp.fc2.weight', 'encoder.layers.5.norm1.bias', 'encoder.layers.5.norm1.weight', 'encoder.layers.5.norm2.bias', 'encoder.layers.5.norm2.weight', 'encoder.layers.6.attn.proj.bias', 'encoder.layers.6.attn.proj.weight', 'encoder.layers.6.attn.qkv.bias', 'encoder.layers.6.attn.qkv.weight', 'encoder.layers.6.ls1', 'encoder.layers.6.ls2', 'encoder.layers.6.mlp.fc1.bias', 'encoder.layers.6.mlp.fc1.weight', 'encoder.layers.6.mlp.fc2.bias', 'encoder.layers.6.mlp.fc2.weight', 'encoder.layers.6.norm1.bias', 'encoder.layers.6.norm1.weight', 'encoder.layers.6.norm2.bias', 'encoder.layers.6.norm2.weight', 'encoder.layers.7.attn.proj.bias', 'encoder.layers.7.attn.proj.weight', 'encoder.layers.7.attn.qkv.bias', 'encoder.layers.7.attn.qkv.weight', 'encoder.layers.7.ls1', 'encoder.layers.7.ls2', 'encoder.layers.7.mlp.fc1.bias', 'encoder.layers.7.mlp.fc1.weight', 'encoder.layers.7.mlp.fc2.bias', 'encoder.layers.7.mlp.fc2.weight', 'encoder.layers.7.norm1.bias', 'encoder.layers.7.norm1.weight', 'encoder.layers.7.norm2.bias', 'encoder.layers.7.norm2.weight', 'encoder.layers.8.attn.proj.bias', 'encoder.layers.8.attn.proj.weight', 'encoder.layers.8.attn.qkv.bias', 'encoder.layers.8.attn.qkv.weight', 'encoder.layers.8.ls1', 'encoder.layers.8.ls2', 'encoder.layers.8.mlp.fc1.bias', 'encoder.layers.8.mlp.fc1.weight', 'encoder.layers.8.mlp.fc2.bias', 'encoder.layers.8.mlp.fc2.weight', 'encoder.layers.8.norm1.bias', 'encoder.layers.8.norm1.weight', 'encoder.layers.8.norm2.bias', 'encoder.layers.8.norm2.weight', 'encoder.layers.9.attn.proj.bias', 'encoder.layers.9.attn.proj.weight', 'encoder.layers.9.attn.qkv.bias', 'encoder.layers.9.attn.qkv.weight', 'encoder.layers.9.ls1', 'encoder.layers.9.ls2', 'encoder.layers.9.mlp.fc1.bias', 'encoder.layers.9.mlp.fc1.weight', 'encoder.layers.9.mlp.fc2.bias', 'encoder.layers.9.mlp.fc2.weight', 'encoder.layers.9.norm1.bias', 'encoder.layers.9.norm1.weight', 'encoder.layers.9.norm2.bias', 'encoder.layers.9.norm2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InternVisionModel(\n",
       "  (embeddings): InternVisionEmbeddings(\n",
       "    (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "  )\n",
       "  (encoder): InternVisionEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x InternVisionEncoderLayer(\n",
       "        (attn): InternAttention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp): InternMLP(\n",
       "          (act): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (drop_path1): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_intern_vit import InternVisionModel\n",
    "\n",
    "checkpoint_path = \"/home/zhangjingbo/Workspace/Mini-Monkey\"\n",
    "model = InternVisionModel.from_pretrained(checkpoint_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type internvl_chat to instantiate a model of type internlm2. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of InternLM2ForCausalLM were not initialized from the model checkpoint at /home/zhangjingbo/Workspace/Mini-Monkey and are newly initialized: ['layers.0.attention.wo.bias', 'layers.0.attention.wo.weight', 'layers.0.attention.wqkv.bias', 'layers.0.attention.wqkv.weight', 'layers.0.attention_norm.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.ffn_norm.weight', 'layers.1.attention.wo.bias', 'layers.1.attention.wo.weight', 'layers.1.attention.wqkv.bias', 'layers.1.attention.wqkv.weight', 'layers.1.attention_norm.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.ffn_norm.weight', 'layers.10.attention.wo.bias', 'layers.10.attention.wo.weight', 'layers.10.attention.wqkv.bias', 'layers.10.attention.wqkv.weight', 'layers.10.attention_norm.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.ffn_norm.weight', 'layers.11.attention.wo.bias', 'layers.11.attention.wo.weight', 'layers.11.attention.wqkv.bias', 'layers.11.attention.wqkv.weight', 'layers.11.attention_norm.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.ffn_norm.weight', 'layers.12.attention.wo.bias', 'layers.12.attention.wo.weight', 'layers.12.attention.wqkv.bias', 'layers.12.attention.wqkv.weight', 'layers.12.attention_norm.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.ffn_norm.weight', 'layers.13.attention.wo.bias', 'layers.13.attention.wo.weight', 'layers.13.attention.wqkv.bias', 'layers.13.attention.wqkv.weight', 'layers.13.attention_norm.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.ffn_norm.weight', 'layers.14.attention.wo.bias', 'layers.14.attention.wo.weight', 'layers.14.attention.wqkv.bias', 'layers.14.attention.wqkv.weight', 'layers.14.attention_norm.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.ffn_norm.weight', 'layers.15.attention.wo.bias', 'layers.15.attention.wo.weight', 'layers.15.attention.wqkv.bias', 'layers.15.attention.wqkv.weight', 'layers.15.attention_norm.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.ffn_norm.weight', 'layers.16.attention.wo.bias', 'layers.16.attention.wo.weight', 'layers.16.attention.wqkv.bias', 'layers.16.attention.wqkv.weight', 'layers.16.attention_norm.weight', 'layers.16.feed_forward.w1.weight', 'layers.16.feed_forward.w2.weight', 'layers.16.feed_forward.w3.weight', 'layers.16.ffn_norm.weight', 'layers.17.attention.wo.bias', 'layers.17.attention.wo.weight', 'layers.17.attention.wqkv.bias', 'layers.17.attention.wqkv.weight', 'layers.17.attention_norm.weight', 'layers.17.feed_forward.w1.weight', 'layers.17.feed_forward.w2.weight', 'layers.17.feed_forward.w3.weight', 'layers.17.ffn_norm.weight', 'layers.18.attention.wo.bias', 'layers.18.attention.wo.weight', 'layers.18.attention.wqkv.bias', 'layers.18.attention.wqkv.weight', 'layers.18.attention_norm.weight', 'layers.18.feed_forward.w1.weight', 'layers.18.feed_forward.w2.weight', 'layers.18.feed_forward.w3.weight', 'layers.18.ffn_norm.weight', 'layers.19.attention.wo.bias', 'layers.19.attention.wo.weight', 'layers.19.attention.wqkv.bias', 'layers.19.attention.wqkv.weight', 'layers.19.attention_norm.weight', 'layers.19.feed_forward.w1.weight', 'layers.19.feed_forward.w2.weight', 'layers.19.feed_forward.w3.weight', 'layers.19.ffn_norm.weight', 'layers.2.attention.wo.bias', 'layers.2.attention.wo.weight', 'layers.2.attention.wqkv.bias', 'layers.2.attention.wqkv.weight', 'layers.2.attention_norm.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.ffn_norm.weight', 'layers.20.attention.wo.bias', 'layers.20.attention.wo.weight', 'layers.20.attention.wqkv.bias', 'layers.20.attention.wqkv.weight', 'layers.20.attention_norm.weight', 'layers.20.feed_forward.w1.weight', 'layers.20.feed_forward.w2.weight', 'layers.20.feed_forward.w3.weight', 'layers.20.ffn_norm.weight', 'layers.21.attention.wo.bias', 'layers.21.attention.wo.weight', 'layers.21.attention.wqkv.bias', 'layers.21.attention.wqkv.weight', 'layers.21.attention_norm.weight', 'layers.21.feed_forward.w1.weight', 'layers.21.feed_forward.w2.weight', 'layers.21.feed_forward.w3.weight', 'layers.21.ffn_norm.weight', 'layers.22.attention.wo.bias', 'layers.22.attention.wo.weight', 'layers.22.attention.wqkv.bias', 'layers.22.attention.wqkv.weight', 'layers.22.attention_norm.weight', 'layers.22.feed_forward.w1.weight', 'layers.22.feed_forward.w2.weight', 'layers.22.feed_forward.w3.weight', 'layers.22.ffn_norm.weight', 'layers.23.attention.wo.bias', 'layers.23.attention.wo.weight', 'layers.23.attention.wqkv.bias', 'layers.23.attention.wqkv.weight', 'layers.23.attention_norm.weight', 'layers.23.feed_forward.w1.weight', 'layers.23.feed_forward.w2.weight', 'layers.23.feed_forward.w3.weight', 'layers.23.ffn_norm.weight', 'layers.24.attention.wo.bias', 'layers.24.attention.wo.weight', 'layers.24.attention.wqkv.bias', 'layers.24.attention.wqkv.weight', 'layers.24.attention_norm.weight', 'layers.24.feed_forward.w1.weight', 'layers.24.feed_forward.w2.weight', 'layers.24.feed_forward.w3.weight', 'layers.24.ffn_norm.weight', 'layers.25.attention.wo.bias', 'layers.25.attention.wo.weight', 'layers.25.attention.wqkv.bias', 'layers.25.attention.wqkv.weight', 'layers.25.attention_norm.weight', 'layers.25.feed_forward.w1.weight', 'layers.25.feed_forward.w2.weight', 'layers.25.feed_forward.w3.weight', 'layers.25.ffn_norm.weight', 'layers.26.attention.wo.bias', 'layers.26.attention.wo.weight', 'layers.26.attention.wqkv.bias', 'layers.26.attention.wqkv.weight', 'layers.26.attention_norm.weight', 'layers.26.feed_forward.w1.weight', 'layers.26.feed_forward.w2.weight', 'layers.26.feed_forward.w3.weight', 'layers.26.ffn_norm.weight', 'layers.27.attention.wo.bias', 'layers.27.attention.wo.weight', 'layers.27.attention.wqkv.bias', 'layers.27.attention.wqkv.weight', 'layers.27.attention_norm.weight', 'layers.27.feed_forward.w1.weight', 'layers.27.feed_forward.w2.weight', 'layers.27.feed_forward.w3.weight', 'layers.27.ffn_norm.weight', 'layers.28.attention.wo.bias', 'layers.28.attention.wo.weight', 'layers.28.attention.wqkv.bias', 'layers.28.attention.wqkv.weight', 'layers.28.attention_norm.weight', 'layers.28.feed_forward.w1.weight', 'layers.28.feed_forward.w2.weight', 'layers.28.feed_forward.w3.weight', 'layers.28.ffn_norm.weight', 'layers.29.attention.wo.bias', 'layers.29.attention.wo.weight', 'layers.29.attention.wqkv.bias', 'layers.29.attention.wqkv.weight', 'layers.29.attention_norm.weight', 'layers.29.feed_forward.w1.weight', 'layers.29.feed_forward.w2.weight', 'layers.29.feed_forward.w3.weight', 'layers.29.ffn_norm.weight', 'layers.3.attention.wo.bias', 'layers.3.attention.wo.weight', 'layers.3.attention.wqkv.bias', 'layers.3.attention.wqkv.weight', 'layers.3.attention_norm.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.ffn_norm.weight', 'layers.30.attention.wo.bias', 'layers.30.attention.wo.weight', 'layers.30.attention.wqkv.bias', 'layers.30.attention.wqkv.weight', 'layers.30.attention_norm.weight', 'layers.30.feed_forward.w1.weight', 'layers.30.feed_forward.w2.weight', 'layers.30.feed_forward.w3.weight', 'layers.30.ffn_norm.weight', 'layers.31.attention.wo.bias', 'layers.31.attention.wo.weight', 'layers.31.attention.wqkv.bias', 'layers.31.attention.wqkv.weight', 'layers.31.attention_norm.weight', 'layers.31.feed_forward.w1.weight', 'layers.31.feed_forward.w2.weight', 'layers.31.feed_forward.w3.weight', 'layers.31.ffn_norm.weight', 'layers.4.attention.wo.bias', 'layers.4.attention.wo.weight', 'layers.4.attention.wqkv.bias', 'layers.4.attention.wqkv.weight', 'layers.4.attention_norm.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.ffn_norm.weight', 'layers.5.attention.wo.bias', 'layers.5.attention.wo.weight', 'layers.5.attention.wqkv.bias', 'layers.5.attention.wqkv.weight', 'layers.5.attention_norm.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.ffn_norm.weight', 'layers.6.attention.wo.bias', 'layers.6.attention.wo.weight', 'layers.6.attention.wqkv.bias', 'layers.6.attention.wqkv.weight', 'layers.6.attention_norm.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.ffn_norm.weight', 'layers.7.attention.wo.bias', 'layers.7.attention.wo.weight', 'layers.7.attention.wqkv.bias', 'layers.7.attention.wqkv.weight', 'layers.7.attention_norm.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.ffn_norm.weight', 'layers.8.attention.wo.bias', 'layers.8.attention.wo.weight', 'layers.8.attention.wqkv.bias', 'layers.8.attention.wqkv.weight', 'layers.8.attention_norm.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.ffn_norm.weight', 'layers.9.attention.wo.bias', 'layers.9.attention.wo.weight', 'layers.9.attention.wqkv.bias', 'layers.9.attention.wqkv.weight', 'layers.9.attention_norm.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.ffn_norm.weight', 'norm.weight', 'output.weight', 'tok_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InternLM2ForCausalLM(\n",
       "  (model): InternLM2Model(\n",
       "    (tok_embeddings): Embedding(103168, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x InternLM2DecoderLayer(\n",
       "        (attention): InternLM2Attention(\n",
       "          (wqkv): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): InternLM2RotaryEmbedding()\n",
       "        )\n",
       "        (feed_forward): InternLM2MLP(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (attention_norm): InternLM2RMSNorm()\n",
       "        (ffn_norm): InternLM2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): InternLM2RMSNorm()\n",
       "  )\n",
       "  (output): Linear(in_features=4096, out_features=103168, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_internlm2 import InternLM2ForSequenceClassification, InternLM2ForCausalLM, InternLM2Model\n",
    "\n",
    "checkpoint_path = \"/home/zhangjingbo/Workspace/Mini-Monkey\"\n",
    "# model = InternLM2Model.from_pretrained(checkpoint_path)\n",
    "model = InternLM2ForCausalLM.from_pretrained(checkpoint_path)\n",
    "# not support yet\n",
    "#model = InternLM2ForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangjingbo/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MiniMonkeyChatModel(\n",
       "  (vision_model): InternVisionModel(\n",
       "    (embeddings): InternVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): InternVisionEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x InternVisionEncoderLayer(\n",
       "          (attn): InternAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (inner_attn): FlashAttention()\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): InternMLP(\n",
       "            (act): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path1): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_model): InternLM2ForCausalLM(\n",
       "    (model): InternLM2Model(\n",
       "      (tok_embeddings): Embedding(92553, 2048, padding_idx=2)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x InternLM2DecoderLayer(\n",
       "          (attention): InternLM2Attention(\n",
       "            (wqkv): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "            (wo): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): InternLM2DynamicNTKScalingRotaryEmbedding()\n",
       "          )\n",
       "          (feed_forward): InternLM2MLP(\n",
       "            (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "            (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (attention_norm): InternLM2RMSNorm()\n",
       "          (ffn_norm): InternLM2RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): InternLM2RMSNorm()\n",
       "    )\n",
       "    (output): Linear(in_features=2048, out_features=92553, bias=False)\n",
       "  )\n",
       "  (mlp1): Sequential(\n",
       "    (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_minimonkey_chat import MiniMonkeyChatModel\n",
    "\n",
    "checkpoint_path = \"/home/zhangjingbo/Workspace/Mini-Monkey\"\n",
    "model = MiniMonkeyChatModel.from_pretrained(checkpoint_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangjingbo/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "architectures ['MiniMonkeyChatModel']\n",
      "architecture_type None\n",
      "model_type internvl_chat\n",
      "yuzhi_version None\n",
      "_name_or_path /home/zhangjingbo/Workspace/Mini-Monkey\n",
      "return_dict True\n",
      "_commit_hash None\n",
      "auto_map {'AutoConfig': 'configuration_internvl_chat.InternVLChatConfig', 'AutoModel': 'modeling_minimonkey_chat.MiniMonkeyChatModel', 'AutoModelForCausalLM': 'modeling_minimonkey_chat.MiniMonkeyChatModel'}\n",
      "torch_dtype bfloat16\n",
      "vision_config {'architectures': ['InternVisionModel'], 'architecture_type': None, 'model_type': 'intern_vit_6b', 'yuzhi_version': '0.0.1', '_name_or_path': '', 'return_dict': True, 'add_cross_attention': False, 'bad_words_ids': None, 'begin_suppress_tokens': None, 'bos_token_id': None, 'chunk_size_feed_forward': 0, 'cross_attention_hidden_size': None, 'decoder_start_token_id': None, 'diversity_penalty': 0.0, 'do_sample': False, 'early_stopping': False, 'encoder_no_repeat_ngram_size': 0, 'eos_token_id': None, 'exponential_decay_length_penalty': None, 'finetuning_task': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1'}, 'is_decoder': False, 'is_encoder_decoder': False, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'length_penalty': 1.0, 'max_length': 20, 'min_length': 0, 'no_repeat_ngram_size': 0, 'num_beam_groups': 1, 'num_beams': 1, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'pad_token_id': None, 'prefix': None, 'problem_type': None, 'pruned_heads': {}, 'remove_invalid_values': False, 'repetition_penalty': 1.0, 'return_dict_in_generate': False, 'sep_token_id': None, 'suppress_tokens': None, 'task_specific_params': None, 'temperature': 1.0, 'tf_legacy_loss': False, 'tie_encoder_decoder': False, 'tie_word_embeddings': True, 'tokenizer_class': None, 'top_k': 50, 'top_p': None, 'torch_dtype': 'bfloat16', 'torchscript': False, 'transformers_version': '4.37.2', 'typical_p': 1.0, 'use_bfloat16': True, 'hidden_size': 1024, 'intermediate_size': 4096, 'dropout': 0.0, 'drop_path_rate': 0.0, 'num_hidden_layers': 24, 'num_attention_heads': 16, 'num_channels': 3, 'patch_size': 14, 'image_size': 448, 'initializer_range': 0.02, 'initializer_factor': 1.0, 'attention_dropout': 0.0, 'layer_norm_eps': 1e-06, 'hidden_act': 'gelu', 'norm_type': 'layer_norm', 'qkv_bias': True, 'qk_normalization': False, 'use_flash_attn': True}\n",
      "llm_config {'vocab_size': 92553, 'max_position_embeddings': 32768, 'hidden_size': 2048, 'intermediate_size': 8192, 'num_hidden_layers': 24, 'num_attention_heads': 16, 'bias': False, 'num_key_value_heads': 8, 'hidden_act': 'silu', 'initializer_range': 0.02, 'rms_norm_eps': 1e-05, 'use_cache': True, 'rope_theta': 1000000, 'rope_scaling': {'factor': 2.0, 'type': 'dynamic'}, 'attn_implementation': 'eager', 'architectures': ['InternLM2ForCausalLM'], 'architecture_type': None, 'model_type': 'internlm2', 'yuzhi_version': '0.0.1', '_name_or_path': './pretrained/internlm2-chat-1_8b', 'return_dict': True, 'pad_token_id': 2, 'bos_token_id': 1, 'eos_token_id': 2, 'tie_word_embeddings': False, 'add_cross_attention': False, 'auto_map': {'AutoConfig': 'configuration_internlm2.InternLM2Config', 'AutoModel': 'modeling_internlm2.InternLM2ForCausalLM', 'AutoModelForCausalLM': 'modeling_internlm2.InternLM2ForCausalLM'}, 'bad_words_ids': None, 'begin_suppress_tokens': None, 'chunk_size_feed_forward': 0, 'cross_attention_hidden_size': None, 'decoder_start_token_id': None, 'diversity_penalty': 0.0, 'do_sample': False, 'early_stopping': False, 'encoder_no_repeat_ngram_size': 0, 'exponential_decay_length_penalty': None, 'finetuning_task': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1'}, 'is_decoder': False, 'is_encoder_decoder': False, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'length_penalty': 1.0, 'max_length': 20, 'min_length': 0, 'no_repeat_ngram_size': 0, 'num_beam_groups': 1, 'num_beams': 1, 'num_return_sequences': 1, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'prefix': None, 'problem_type': None, 'pruned_heads': {}, 'remove_invalid_values': False, 'repetition_penalty': 1.0, 'return_dict_in_generate': False, 'sep_token_id': None, 'suppress_tokens': None, 'task_specific_params': None, 'temperature': 1.0, 'tf_legacy_loss': False, 'tie_encoder_decoder': False, 'tokenizer_class': None, 'top_k': 50, 'top_p': None, 'torch_dtype': 'bfloat16', 'torchscript': False, 'transformers_version': '4.37.2', 'typical_p': 1.0, 'use_bfloat16': True}\n",
      "use_backbone_lora 0\n",
      "use_llm_lora 0\n",
      "pad2square False\n",
      "select_layer -1\n",
      "force_image_size 448\n",
      "downsample_ratio 0.5\n",
      "template internlm2-chat\n",
      "dynamic_image_size True\n",
      "use_thumbnail True\n",
      "ps_version v2\n",
      "min_dynamic_patch 1\n",
      "max_dynamic_patch 12\n",
      "_attn_implementation eager\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (CUDABFloat16Type) and weight type (CPUBFloat16Type) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 169\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m#question = \"Read the all text in the image.\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m#question = \"Describe the image.\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 169\u001b[0m response, history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Assistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# '''\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/yu/yu-zhi/src/yuzhi/model/models/mini_monkey/modeling_minimonkey_chat.py:285\u001b[0m, in \u001b[0;36mMiniMonkeyChatModel.chat\u001b[0;34m(self, tokenizer, pixel_values, target_aspect_ratio, question, generation_config, use_scm, history, return_history, num_patches_list, IMG_START_TOKEN, IMG_END_TOKEN, IMG_CONTEXT_TOKEN, verbose)\u001b[0m\n\u001b[1;32m    283\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m model_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    284\u001b[0m generation_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meos_token_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m eos_token_id\n\u001b[0;32m--> 285\u001b[0m generation_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_scm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_scm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generation_output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    294\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39msplit(template\u001b[38;5;241m.\u001b[39msep)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/yu/yu-zhi/src/yuzhi/model/models/mini_monkey/modeling_minimonkey_chat.py:325\u001b[0m, in \u001b[0;36mMiniMonkeyChatModel.generate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, target_aspect_ratio, visual_features, generation_config, output_hidden_states, return_dict, use_scm, **generate_kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m     vit_embeds \u001b[38;5;241m=\u001b[39m visual_features\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     vit_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m input_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model\u001b[38;5;241m.\u001b[39mget_input_embeddings()(input_ids)\n\u001b[1;32m    327\u001b[0m B, N, C \u001b[38;5;241m=\u001b[39m input_embeds\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Workspace/yu/yu-zhi/src/yuzhi/model/models/mini_monkey/modeling_minimonkey_chat.py:181\u001b[0m, in \u001b[0;36mMiniMonkeyChatModel.extract_feature\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_feature\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_layer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 181\u001b[0m         vit_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m         vit_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model(\n\u001b[1;32m    187\u001b[0m             pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[1;32m    188\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    189\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_layer]\n",
      "File \u001b[0;32m~/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/yu/yu-zhi/src/yuzhi/model/models/mini_monkey/modeling_intern_vit.py:413\u001b[0m, in \u001b[0;36mInternVisionModel.forward\u001b[0;34m(self, pixel_values, output_hidden_states, return_dict, pixel_embeds)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pixel_values\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m--> 413\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrong pixel_values size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixel_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/yu/yu-zhi/src/yuzhi/model/models/mini_monkey/modeling_intern_vit.py:167\u001b[0m, in \u001b[0;36mInternVisionEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values: torch\u001b[38;5;241m.\u001b[39mFloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    166\u001b[0m     target_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 167\u001b[0m     patch_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape = [*, channel, width, height]\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     batch_size, _, height, width \u001b[38;5;241m=\u001b[39m patch_embeds\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    169\u001b[0m     patch_embeds \u001b[38;5;241m=\u001b[39m patch_embeds\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/yuzhi_dev/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (CUDABFloat16Type) and weight type (CPUBFloat16Type) should be the same"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from modeling_minimonkey_chat import MiniMonkeyChatModel\n",
    "from tokenization_internlm2 import InternLM2Tokenizer\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images, target_aspect_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess2(image, min_num=1, max_num=12, prior_aspect_ratio=None, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "    new_target_ratios = []\n",
    "    for i in target_ratios:\n",
    "        if prior_aspect_ratio[0]%i[0] or prior_aspect_ratio[1]%i[1]:\n",
    "            new_target_ratios.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, new_target_ratios, orig_width, orig_height, image_size)\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, min_num=1, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images, target_aspect_ratio = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, min_num=min_num, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values, target_aspect_ratio\n",
    "\n",
    "def load_image2(image_file, input_size=448, min_num=1, max_num=12, target_aspect_ratio=None):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess2(image, image_size=input_size, use_thumbnail=True, min_num=min_num, max_num=max_num, prior_aspect_ratio=target_aspect_ratio)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "# If you want to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\n",
    "# path = 'minimonkey'\n",
    "path = '/home/zhangjingbo/Workspace/Mini-Monkey'\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     path,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     trust_remote_code=True).eval().cuda()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "model = MiniMonkeyChatModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16).eval().cuda()\n",
    "#.cuda()\n",
    "tokenizer = InternLM2Tokenizer.from_pretrained(path)\n",
    "\n",
    "# print(model.config.attribute_map)\n",
    "# for k, v in model.config.to_dict().items():\n",
    "#     print(k, v)\n",
    "\n",
    "# '''\n",
    "image_path = '/home/zhangjingbo/Workspace/data/minimonkey/movie_cap.png'\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values, target_aspect_ratio = load_image(image_path, min_num=4, max_num=12)\n",
    "pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image2(image_path, min_num=3, max_num=7, target_aspect_ratio=target_aspect_ratio)\n",
    "pixel_values2 = pixel_values2.to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat([pixel_values[:-1], pixel_values2], 0)\n",
    "\n",
    "generation_config = dict(do_sample=False, max_new_tokens=512)\n",
    "\n",
    "#question = \"Read the all text in the image.\"\n",
    "#question = \"Describe the image.\"\n",
    "question = \"\"\n",
    "response, history = model.chat(tokenizer, pixel_values, target_aspect_ratio, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question} Assistant: {response}')\n",
    "# '''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuzhi_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
